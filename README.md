Matrix size	   Serial (s)	   1 proc(s)	  Speedup	    2 proc(s)	  Speedup	  4 proc(s)	 Speedup	  6 proc(s)	 Speedup	  8 proc(s)	  Speedup	  16 proc(s)	Speedup
10	            0,000001	 0,000011	     0,000012	    0,000069	0,014492754	0,000089	0,011235955	0,000138	0,007246377	0,000165	0,006060606	0,038657	2,58685E-05
100	            0,000032	 0,000072	     0,444444444	0,000138	0,231884058	0,000103	0,310679612	0,000156	0,205128205	0,000464	0,068965517	0,031815	0,001005815
1000	          0,002448	 0,005234	     0,46771112	  0,006493	0,377021408	0,020615	0,118748484	0,008894	0,275241736	0,006867	0,356487549	0,016945	0,144467395
2000	          0,010333	 0,015779	     0,654857722	0,026409	0,391268128	0,02478	  0,416989508	0,030434	0,339521588	0,037641	0,274514492	0,023246	0,444506582
3000	          0,022989	 0,054325	     0,423175334	0,05672	  0,40530677	0,074629	0,30804379	0,067067	0,342776626	0,080357	0,286085842	0,080562	0,285357861
4000	          0,040702	 0,101507	     0,400977273	0,100118	0,406540282	0,113142	0,359742624	0,126749	0,321122849	0,132633	0,306876871	0,116659	0,348897213
5000	          0,062956	 0,152321	     0,413311362	0,151803	0,414721712	0,165501	0,380396493	0,19697	  0,319622278	0,22769	  0,276498748	0,093704	0,671860326
6000	          0,091669	 0,23459	     0,390762607	0,239289	0,383089068	0,255732	0,358457291	0,29442	  0,311354528	0,299779	0,305788598	0,152489	0,601151558
7000	          0,11916	   0,305784	     0,389686838	0,323325	0,368545581	0,326738	0,364695873	0,424265	0,280862197	0,469643	0,253724638	0,23091	  0,516045212
8000	          0,164484	 0,378985	     0,4340119	  0,39882	  0,412426659	0,407062	0,404076038	0,530118	0,310278089	0,518796	0,317049476	0,201675	0,815589438
9000	          0,199298	 0,518484	     0,384386018	0,550995	0,361705642	0,593616	0,33573556	0,663042	0,30058126	0,727672	0,273884387	0,265267	0,751310943
10000	          0,241401	 0,646909	     0,373160676	0,629267	0,383622532	0,737513	0,32731762	0,872301	0,276740483	1,808779	0,133460749	0,353339	0,68319942

Висновок
У ході роботи було реалізовано та проаналізовано два підходи до задачі множення матриці на вектор: послідовний (файл SerialMV.cpp) та паралельний з використанням технології MPI (файли ParallelMV.cpp та TestParallelMV.cpp).
Аналіз реалізованих алгоритмів:
•	Послідовний алгоритм (SerialMV.cpp):
Використовує стандартний підхід з двома вкладеними циклами, що має обчислювальну складність O(n^2), де n — розмірність матриці та вектора.
•	Паралельний алгоритм (ParallelMV.cpp):
Реалізовано стратегію 1D-декомпозиції даних (розбиття матриці за рядками).
1.	Ініціалізація: Кореневий процес (rank 0) ініціалізує повну матрицю та вектор.
2.	Розподіл даних:
Повний вектор розсилається всім процесам за допомогою MPI_Bcast.
Матриця розподіляється по рядках (смугах) між усіма процесами за допомогою MPI_Scatterv.
3.	Обчислення: Кожен процес паралельно виконує множення своєї смуги матриці на повний вектор, обчислюючи частину результуючого вектора.
4.	Збір результатів: Часткові результати збираються в один повний вектор на всіх процесах за допомогою MPI_Allgatherv.
Аналіз експериментальних результатів:
Тестування, проведене за допомогою TestParallelMV.cpp на різних розмірах матриць (від 10x10 до 10000x10000), дозволяє зробити наступні висновки, які збігаються з наданими тезами:
1.	Ефективність при малих розмірах: Паралелізація не завжди гарантує прискорення. Для матриць малого розміру послідовна версія виявилася швидшою. Це пояснюється тим, що накладні витрати на обмін даними (час на виконання MPI_Bcast, MPI_Scatterv, MPI_Allgatherv) та синхронізацію процесів MPI домінують над фактичним часом обчислень.
2.	Ефективність при великих розмірах: Зі збільшенням розміру матриці (наприклад, n > 1000$), обчислювальне навантаження O(n^2) стає вагомішим порівняно з комунікаційними витратами. Розподіл цього навантаження між процесами (O(n^2/p)) дозволяє паралельному алгоритму працювати швидше за послідовну реалізацію.
3.	Сублінійне прискорення: Досягнуте прискорення залишається сублінійним (тобто, при використанні p процесів, прискорення менше ніж у p разів). Це очікуваний результат, оскільки неминучі витрати на комунікацію та синхронізацію є частиною загального часу виконання паралельної програми (відповідно до закону Амдала).
4.	Масштабованість: Було помічено, що для великих матриць найкраща продуктивність спостерігалась при певній кількості процесів (наприклад, 8–16). При подальшому збільшенні кількості процесів накладні витрати на обмін даними та синхронізацію між великою кількістю учасників знову починають переважати вигоди від зменшення обчислювального навантаження на кожному окремому ядрі.

Робота підтвердила, що ефективність паралельних обчислень критично залежить від співвідношення "обчислення/комунікація", яке, в свою чергу, залежить від розміру задачі та кількості задіяних обчислювальних ресурсів.
