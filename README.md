

Висновок
У ході роботи було реалізовано та проаналізовано два підходи до задачі множення матриці на вектор: послідовний (файл SerialMV.cpp) та паралельний з використанням технології MPI (файли ParallelMV.cpp та TestParallelMV.cpp).
Аналіз реалізованих алгоритмів:
•	Послідовний алгоритм (SerialMV.cpp):
Використовує стандартний підхід з двома вкладеними циклами, що має обчислювальну складність O(n^2), де n — розмірність матриці та вектора.
•	Паралельний алгоритм (ParallelMV.cpp):
Реалізовано стратегію 1D-декомпозиції даних (розбиття матриці за рядками).
1.	Ініціалізація: Кореневий процес (rank 0) ініціалізує повну матрицю та вектор.
2.	Розподіл даних:
Повний вектор розсилається всім процесам за допомогою MPI_Bcast.
Матриця розподіляється по рядках (смугах) між усіма процесами за допомогою MPI_Scatterv.
3.	Обчислення: Кожен процес паралельно виконує множення своєї смуги матриці на повний вектор, обчислюючи частину результуючого вектора.
4.	Збір результатів: Часткові результати збираються в один повний вектор на всіх процесах за допомогою MPI_Allgatherv.
Аналіз експериментальних результатів:
Тестування, проведене за допомогою TestParallelMV.cpp на різних розмірах матриць (від 10x10 до 10000x10000), дозволяє зробити наступні висновки, які збігаються з наданими тезами:
1.	Ефективність при малих розмірах: Паралелізація не завжди гарантує прискорення. Для матриць малого розміру послідовна версія виявилася швидшою. Це пояснюється тим, що накладні витрати на обмін даними (час на виконання MPI_Bcast, MPI_Scatterv, MPI_Allgatherv) та синхронізацію процесів MPI домінують над фактичним часом обчислень.
2.	Ефективність при великих розмірах: Зі збільшенням розміру матриці (наприклад, n > 1000$), обчислювальне навантаження O(n^2) стає вагомішим порівняно з комунікаційними витратами. Розподіл цього навантаження між процесами (O(n^2/p)) дозволяє паралельному алгоритму працювати швидше за послідовну реалізацію.
3.	Сублінійне прискорення: Досягнуте прискорення залишається сублінійним (тобто, при використанні p процесів, прискорення менше ніж у p разів). Це очікуваний результат, оскільки неминучі витрати на комунікацію та синхронізацію є частиною загального часу виконання паралельної програми (відповідно до закону Амдала).
4.	Масштабованість: Було помічено, що для великих матриць найкраща продуктивність спостерігалась при певній кількості процесів (наприклад, 8–16). При подальшому збільшенні кількості процесів накладні витрати на обмін даними та синхронізацію між великою кількістю учасників знову починають переважати вигоди від зменшення обчислювального навантаження на кожному окремому ядрі.

Робота підтвердила, що ефективність паралельних обчислень критично залежить від співвідношення "обчислення/комунікація", яке, в свою чергу, залежить від розміру задачі та кількості задіяних обчислювальних ресурсів.
